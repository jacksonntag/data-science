---
title: Distributed Computing and Spark
author: Thinkful
team: grading
time: 45
uuid: e8b1ac5c-c7c0-4af0-8b27-b26440aadd12
---

So we’ve covered the software for accessing data that is in a distributed form. Now you want to do something with it. What can you do about it? Chances are your analyses are probably going to be pretty slow if you try to do them all at once on a single machine as a single job.

There are several ways to deal with this issue. We’ll focus on two. One of them is local, the other distributed.

## Multi-Core Computing

When you bought your computer, you may have heard about how many cores it has. Home computers used to all be single core machines, one place where everything ran from a CPU perspective. Now they are typically multi-core machines, usually either 2, 4, or 8.

If you have a multi-core machine then there is a way you can build models more quickly. When building a model in SKLearn, one of the options you have is ‘n_jobs’. This is the number of different cores your model training can run on. The higher this number, the more distributed the process of building the model will be. This is referred to as parallelization. The training will run on multiple cores simultaneously.  Ideally, this will mean that processing time is divided by the number of cores, so that splitting across two cores means an analysis will run in half the time.

Some data science models are easier to parallelize than others, most can be parallelized in some way. Random forest is probably the easiest to see how it can be parallelized: different trees are generated by different cores. Boosted trees can also easily be parallelized when, as the tree starts to split, subsequent models are run in different cores. Something like SVM doesn’t particularly parallelize, but luckily memory isn’t much of a problem since we only care about points near the margins.

## SPARK

Sometimes, actually often, you just can’t run the model on your local machine. It’s either too much data or would take too long to train. In that case you need to use something else. That something else is Spark. Spark is part of the Apache suite that has been built up around Hadoop.

Spark is a fantastic tool for distributed computing. And lucky for us it is incredibly easy to translate our python code into spark. That’s because Spark has been translated into python like syntax with [PySpark](https://spark.apache.org/docs/0.9.0/python-programming-guide.html) and there is even a Spark version of iPython notebooks and Sci-Kit Learn.

PySpark will look almost identical to Python. You just need infrastructure set up to run it in a distributed fashion.