---
time: 120
name: 'Challenge: Data cleaning & validation'
author: Bethany Kok

---

Data cleaning is definitely a "practice makes perfect" skill.  Using this dataset of article open-access prices paid by the [WELLCOME Trust between 2012 and 2013](https://www.dropbox.com/s/pl5kcrhs2lyj90m/WELLCOME.zip?dl=0), determine the five most common journals and the total articles for each.  Next, calculate the mean, median, and standard deviation of the open-access cost per article for each journal . You will need to do considerable data cleaning in order to extract accurate estimates, and may want to look into data [encoding methods](https://stackoverflow.com/questions/2241348/what-is-unicode-utf-8-utf-16) if you get stuck. For a real bonus round, identify the open access prices paid by subject area.

Remember not to modify the data directly. Instead, write a cleaning script that will load the raw data and whip it into shape. Jupyter notebooks are a great format for this. Keep a record of your decisions: well-commented code is a must for recording your data cleaning decision-making progress. Submit a link to your script and results below and discuss it with your mentor at your next session.

